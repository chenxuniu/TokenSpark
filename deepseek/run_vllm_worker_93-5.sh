#!/bin/bash
#SBATCH --job-name=vllm-worker3-fixed
#SBATCH --partition=h100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:4
#SBATCH --mem=32G
#SBATCH --time=24:00:00
#SBATCH --output=vllm_worker3_fixed_%j.out
#SBATCH --error=vllm_worker3_fixed_%j.err
#SBATCH --nodelist=rpg-93-5  # ğŸ”¥ æ–°èŠ‚ç‚¹
#SBATCH --exclusive

# ä¿®å¤ç‰ˆvLLMåˆ†å¸ƒå¼æ¨ç† - Worker3èŠ‚ç‚¹ (é•¿æ—¶é—´è¿è¡Œç‰ˆ)

echo "=== ä¿®å¤ç‰ˆvLLMåˆ†å¸ƒå¼æ¨ç† - Worker3èŠ‚ç‚¹ (é•¿æ—¶é—´è¿è¡Œ) ==="
echo "Job: $SLURM_JOB_ID"
echo "Node: $(hostname)"
echo "Date: $(date)"

# Setup environment
source ~/.bashrc
cd $HOME/uv && source vllm_benchmark/bin/activate
cd $HOME/codes/ray_vllm_405B_alpaca

# ğŸ”¥ æ¸…ç†ç¼“å­˜
echo "ğŸ§¹ Cleaning vLLM cache..."
rm -rf ~/.cache/vllm/torch_compile_cache
rm -rf ~/.cache/vllm/
rm -rf ~/.cache/torch/compile_cache  
rm -rf ~/.cache/triton
echo "âœ… Cache cleanup completed"

# æ£€æŸ¥CUDAç¯å¢ƒ
echo "ğŸ”§ Checking CUDA environment..."
echo "CUDA_VISIBLE_DEVICES: '$CUDA_VISIBLE_DEVICES'"
nvidia-smi --query-gpu=index,name --format=csv,noheader

# ç¡®ä¿CUDA_VISIBLE_DEVICESæ­£ç¡®è®¾ç½®
if [ -z "$CUDA_VISIBLE_DEVICES" ]; then
    export CUDA_VISIBLE_DEVICES="0,1,2,3"
    echo "Set CUDA_VISIBLE_DEVICES to: $CUDA_VISIBLE_DEVICES"
fi

# Clean up existing Ray processes
echo "ğŸ§¹ Cleaning up existing Ray processes..."
ray stop --force 2>/dev/null || true
pkill -f "ray::" 2>/dev/null || true
sleep 5

# Configuration
head_ip="10.100.93.6"
port=6379
ray_address="$head_ip:$port"

echo "Worker3 connecting to: $ray_address"

# Test connectivity
echo ""
echo "ğŸ” Testing connectivity..."
if ping -c 3 $head_ip > /dev/null 2>&1; then
    echo "âœ… Head node reachable"
else
    echo "âŒ Head node unreachable"
    exit 1
fi

# Wait for head node to be ready
echo "â³ Waiting for head node to be ready..."
max_attempts=10
attempt=1

while [ $attempt -le $max_attempts ]; do
    echo "--- Connection attempt $attempt/$max_attempts ---"
    
    if timeout 10 bash -c "</dev/tcp/$head_ip/$port" 2>/dev/null; then
        echo "âœ… Head node is ready"
        break
    else
        echo "â³ Head node not ready, waiting..."
        sleep 20
        ((attempt++))
    fi
done

if [ $attempt -gt $max_attempts ]; then
    echo "âŒ Head node never became ready"
    exit 1
fi

# Start Ray worker
echo ""
echo "ğŸš€ Starting Ray worker3..."

ray start --address="$ray_address" \
    --num-cpus=8 \
    --num-gpus=4 \
    --disable-usage-stats \
    --object-store-memory=3000000000

if [ $? -eq 0 ]; then
    echo "âœ… Ray worker3 connected successfully!"
else
    echo "âŒ Ray worker3 failed to connect"
    exit 1
fi

# Verify connection
echo ""
echo "ğŸ§ª Verifying worker3 connection..."

python << 'EOF'
import ray
import time

try:
    print("ğŸ”— Connecting to verify worker3...")
    ray.init(address="10.100.93.6:6379", ignore_reinit_error=True)
    
    # Get cluster info
    nodes = ray.nodes()
    resources = ray.cluster_resources()
    alive_nodes = [n for n in nodes if n['Alive']]
    
    print(f"âœ… Worker3 verification:")
    print(f"   Total nodes: {len(alive_nodes)}")
    print(f"   Total GPUs: {int(resources.get('GPU', 0))}")
    print(f"   Total CPUs: {int(resources.get('CPU', 0))}")
    
    # Check if this worker is recognized
    worker_found = False
    for node in alive_nodes:
        hostname = node.get('NodeManagerHostname', '')
        if 'rpg-93-5' in hostname:
            worker_found = True
            node_res = node.get('Resources', {})
            print(f"   This worker3: {hostname} - {int(node_res.get('GPU', 0))} GPUs")
            break
    
    if worker_found:
        print("âœ… Worker3 properly registered in cluster")
    else:
        print("âš ï¸  Worker3 not found in cluster")
    
    ray.shutdown()
    
except Exception as e:
    print(f"âŒ Worker3 verification failed: {e}")
    import traceback
    traceback.print_exc()
EOF

if [ $? -eq 0 ]; then
    echo "âœ… Worker3 verification passed!"
else
    echo "âš ï¸  Worker3 verification had issues"
fi

# FIXED: é•¿æ—¶é—´ä¿æŒæ´»è·ƒçŠ¶æ€
echo ""
echo "ğŸ’¤ Worker3 staying alive for the full job duration..."
echo "Ready for vLLM distributed inference"
echo "ğŸ”¥ Worker3 will remain active until SLURM job ends or head node stops"

# åˆ›å»ºçŠ¶æ€æ–‡ä»¶æ¥è·Ÿè¸ªworker3çŠ¶æ€
worker3_status_file="/tmp/vllm_worker3_status_$SLURM_JOB_ID"
echo "active" > "$worker3_status_file"

# å‡½æ•°ï¼šæ£€æŸ¥head nodeæ˜¯å¦è¿˜åœ¨è¿è¡Œ
check_head_node() {
    python -c "
import ray
import sys
try:
    ray.init(address='10.100.93.6:6379', ignore_reinit_error=True, _temp_dir='/tmp/ray_temp_worker3')
    nodes = ray.nodes()
    alive_nodes = [n for n in nodes if n['Alive']]
    head_active = any('rpg-93-6' in n.get('NodeManagerHostname', '') for n in alive_nodes)
    
    if head_active:
        resources = ray.cluster_resources()
        worker3_active = any('rpg-93-5' in n.get('NodeManagerHostname', '') for n in alive_nodes)
        print(f'OK - Cluster: {len(alive_nodes)} nodes, {int(resources.get(\"GPU\", 0))} GPUs, Head: {head_active}, Worker3: {worker3_active}')
        sys.exit(0)
    else:
        print('HEAD_DOWN - Head node is not active')
        sys.exit(1)
    
    ray.shutdown()
except Exception as e:
    print(f'ERROR - Connection failed: {e}')
    sys.exit(2)
" 2>/dev/null
}

# ç›‘æ§å¾ªç¯ - æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œç›´åˆ°SLURMä»»åŠ¡ç»“æŸ
check_count=0
max_inactive_checks=10  # æœ€å¤šå…è®¸10æ¬¡è¿ç»­å¤±è´¥
inactive_count=0

while [ -f "$worker3_status_file" ]; do
    check_count=$((check_count + 1))
    current_time=$(date '+%Y-%m-%d %H:%M:%S')
    
    # æ¯åˆ†é’Ÿæ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€ï¼ˆæ¯30ç§’æ£€æŸ¥ï¼Œæ¯2æ¬¡æ˜¾ç¤ºï¼‰
    if [ $((check_count % 2)) -eq 0 ]; then
        echo "--- Worker3 status check #$((check_count/2)) ($current_time) ---"
        
        # æ£€æŸ¥head nodeçŠ¶æ€
        status_output=$(check_head_node)
        status_code=$?
        
        if [ $status_code -eq 0 ]; then
            echo "   $status_output"
            inactive_count=0  # é‡ç½®å¤±è´¥è®¡æ•°
        elif [ $status_code -eq 1 ]; then
            echo "   $status_output"
            echo "   Head node is down, worker3 will continue waiting..."
            inactive_count=$((inactive_count + 1))
        else
            echo "   $status_output"
            inactive_count=$((inactive_count + 1))
        fi
        
        # å¦‚æœè¿ç»­å¤±è´¥å¤ªå¤šæ¬¡ï¼Œè€ƒè™‘é€€å‡º
        if [ $inactive_count -ge $max_inactive_checks ]; then
            echo "   âš ï¸  Head node has been inactive for too long, but worker3 will keep waiting..."
            # å¯ä»¥é€‰æ‹©ç»§ç»­ç­‰å¾…æˆ–é€€å‡ºï¼Œè¿™é‡Œé€‰æ‹©ç»§ç»­ç­‰å¾…
            # break
        fi
        
        # æ˜¾ç¤ºå‰©ä½™æ—¶é—´ä¿¡æ¯
        job_start_time=$(squeue -j $SLURM_JOB_ID -h -o %S 2>/dev/null)
        if [ -n "$job_start_time" ]; then
            echo "   ğŸ’¡ Worker3 active since job start. Use 'scancel $SLURM_JOB_ID' to stop if needed."
        fi
        
        # æ˜¾ç¤ºå½“å‰é›†ç¾¤çŠ¶æ€çš„ç®€è¦ä¿¡æ¯
        cluster_info=$(python -c "
import ray
try:
    ray.init(address='10.100.93.6:6379', ignore_reinit_error=True, _temp_dir='/tmp/ray_temp_worker3_brief')
    nodes = ray.nodes()
    alive_nodes = [n for n in nodes if n['Alive']]
    
    # ç»Ÿè®¡å„èŠ‚ç‚¹GPUæ•°é‡
    node_gpus = {}
    for node in alive_nodes:
        hostname = node.get('NodeManagerHostname', 'unknown')
        if 'rpg-93-6' in hostname:
            node_name = 'head'
        elif 'rpg-93-8' in hostname:
            node_name = 'worker2'
        elif 'rpg-93-5' in hostname:
            node_name = 'worker3'
        else:
            node_name = hostname
        
        node_res = node.get('Resources', {})
        node_gpus[node_name] = int(node_res.get('GPU', 0))
    
    gpu_summary = ', '.join([f'{k}:{v}GPU' for k, v in node_gpus.items()])
    print(f'   Nodes active: {gpu_summary}')
    
    ray.shutdown()
except Exception:
    print('   Cluster info unavailable')
" 2>/dev/null)
        
        if [ -n "$cluster_info" ]; then
            echo "$cluster_info"
        fi
    fi
    
    # æ£€æŸ¥SLURMä»»åŠ¡æ˜¯å¦è¿˜åœ¨è¿è¡Œ
    if ! squeue -j $SLURM_JOB_ID >/dev/null 2>&1; then
        echo "   ğŸ›‘ SLURM job $SLURM_JOB_ID no longer in queue, worker3 will exit"
        break
    fi
    
    # ç­‰å¾…30ç§’åç»§ç»­æ£€æŸ¥
    sleep 30
done

# æ¸…ç†å¹¶é€€å‡º
echo ""
echo "ğŸ›‘ Worker3 shutting down gracefully..."
echo "   Final status check before cleanup:"

# æœ€åä¸€æ¬¡çŠ¶æ€æ£€æŸ¥
final_status=$(check_head_node)
echo "   $final_status"

# æ¸…ç†çŠ¶æ€æ–‡ä»¶
rm -f "$worker3_status_file"

# åœæ­¢Ray worker
echo "   Stopping Ray worker3..."
ray stop --force

echo "âœ… Worker3 job completed gracefully at $(date)"
echo "   Job ID: $SLURM_JOB_ID"
echo "   Node: rpg-93-5"
echo "   Total checks performed: $((check_count/2))"